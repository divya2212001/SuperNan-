# -*- coding: utf-8 -*-
"""Supernan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EzHuot-u90JuO6_BlXymQpLDjIW_UNKT
"""

!nvidia-smi

import torch
torch.cuda.is_available()

import os

# System dependencies
!apt-get update -y
!apt-get install -y ffmpeg

# Core Python libraries
!pip install -q openai-whisper faster-whisper gdown

# Translation + Text processing
!pip install -q transformers sentencepiece

# Voice cloning (Coqui XTTS)
!pip install -q TTS

# Lip sync (Wav2Lip) and optional face restoration
if not os.path.exists("Wav2Lip"):
    !git clone https://github.com/Rudrabha/Wav2Lip.git
!pip install -r Wav2Lip/requirements.txt
!pip install -q gfpgan

!pip install gdown

!gdown --id 1aqolvo-SutfS9HIRL2LDP79pBOJdZPMI -O input.mp4

import os

def extract_chunk(input_path, output_path):
    os.system(f"ffmpeg -i {input_path} -ss 00:00:15 -to 00:00:30 -c copy {output_path}")

extract_chunk("input.mp4", "chunk.mp4")

def extract_audio(video_path, audio_path):
    os.system(f"ffmpeg -i {video_path} -q:a 0 -map a {audio_path}")

extract_audio("chunk.mp4", "audio.wav")

from faster_whisper import WhisperModel

def transcribe_auto(audio_path):
    # Changed compute_type to "float32" for broader compatibility
    model = WhisperModel("medium", compute_type="float32")

    segments, info = model.transcribe(
        audio_path,
        task="translate"   # Always translate to English
    )

    print("Detected language:", info.language)
    print("Language probability:", info.language_probability)

    full_text = ""
    for segment in segments:
        full_text += segment.text + " "

    return full_text.strip()

transcript_en = transcribe_auto("audio.wav")
print("\nEnglish Transcript:\n", transcript_en)

!pip install -q transformers sentencepiece accelerate

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
import torch

# Load model once (outside function ideally)
model_name = "facebook/nllb-200-distilled-600M"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

def translate_to_hindi(text):
    # NLLB language codes
    src_lang = "eng_Latn"
    tgt_lang = "hin_Deva"

    tokenizer.src_lang = src_lang

    inputs = tokenizer(text, return_tensors="pt", truncation=True)

    with torch.no_grad():
        generated_tokens = model.generate(
            **inputs,
            forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),
            max_length=512
        )

    translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
    return translated_text


# Use your English transcript
hindi_text = translate_to_hindi(transcript_en)

print("Hindi Translation:\n")
print(hindi_text)

!pip install edge-tts

import edge_tts
import asyncio

async def generate_hindi_speech(text, output_path="hindi_speech.wav"):
    voice = "hi-IN-SwaraNeural"
    communicate = edge_tts.Communicate(text, voice)
    await communicate.save(output_path)

# Removed asyncio.run() and used await directly
await generate_hindi_speech(hindi_text)

print("Hindi speech generated.")

!ls

import librosa
import os

orig_duration = librosa.get_duration(path="audio.wav")
hindi_duration = librosa.get_duration(path="hindi_speech.wav")

print("Original:", orig_duration)
print("Hindi:", hindi_duration)

speed_factor = hindi_duration / orig_duration  # slow down Hindi

os.system(f"ffmpeg -y -i hindi_speech.wav -filter:a 'atempo={speed_factor}' adjusted_hindi.wav")

adjusted_duration = librosa.get_duration(path="adjusted_hindi.wav")
print("Adjusted:", adjusted_duration)

!ffmpeg -y \
-i chunk.mp4 \
-i adjusted_hindi.wav \
-c:v copy \
-map 0:v:0 \
-map 1:a:0 \
-shortest \
final_output.mp4

!ls

